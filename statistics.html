<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Statistics – Intelligent Agents e-Portfolio</title>
  <link rel="stylesheet" href="style.css" />
</head>

<body id="top">
  <a class="skip-link" href="#main">Skip to content</a>

  <header class="site-header">
    <div class="header-inner">
      <div class="brand">
        <h1>Intelligent Agents</h1>
        <div class="sub">MSc Artificial Intelligence – e-Portfolio</div>
      </div>

      <nav class="nav" aria-label="Main navigation">
        <a href="index.html">Home</a>
        <a href="discussion.html">Discussions</a>
        <a href="activities.html">Activities</a>
        <a class="active" href="statistics.html" aria-current="page">Statistics</a>
        <a href="reflections.html">Reflections</a>
        <a href="about.html">About</a>
      </nav>
    </div>
  </header>

  <main id="main" class="container">

    <section class="hero">
      <h2>Statistics</h2>
      <p>
        This page demonstrates quantitative thinking and evidence-based evaluation relevant to intelligent agents and
        multi-agent systems. It includes small, clear analyses aligned with agent performance evaluation, fairness,
        reliability, and decision-making under uncertainty.
      </p>
      <div class="meta">
        <span class="badge">Evaluation</span>
        <span class="badge">Metrics</span>
        <span class="badge">Interpretation</span>
      </div>
    </section>

    <section class="grid">

      <div class="card prose">
        <h3>1) Why statistics matters for Intelligent Agents</h3>
        <p>
          Intelligent agents operate in uncertain environments and can produce variable outcomes depending on state,
          data, interactions, and stochastic policies. Statistics supports:
        </p>
        <ul>
          <li><strong>Performance evaluation</strong> (accuracy, reward, latency, success rate).</li>
          <li><strong>Reliability</strong> (variance, confidence intervals, stability across runs).</li>
          <li><strong>Comparison</strong> (baseline vs. improved agent policy; A/B-style evaluation).</li>
          <li><strong>Risk and fairness assessment</strong> (error rates across groups and contexts).</li>
        </ul>

        <div class="refs">
          <h4>References</h4>
          <ul>
            <li>Russell, S. and Norvig, P. (2021) <em>Artificial Intelligence: A Modern Approach</em>. 4th edn. Harlow: Pearson.</li>
            <li>Wooldridge, M. (2021) <em>An Introduction to MultiAgent Systems</em>. 2nd edn. Chichester: John Wiley &amp; Sons.</li>
          </ul>
        </div>
      </div>

      <div class="card prose">
        <h3>2) Example metrics for agent evaluation</h3>
        <p class="muted">
          The goal here is to show appropriate metrics and how they should be interpreted. Values are presented as
          example templates you can keep (or replace with your real results if you have them later).
        </p>

        <div class="kv">
          <div>Task outcome metrics</div>
          <div>Success rate, completion time, failure rate, throughput.</div>

          <div>Quality metrics</div>
          <div>Reward (mean cumulative), accuracy / F1 (if classification is involved), cost savings.</div>

          <div>Stability metrics</div>
          <div>Variance across runs, confidence intervals, sensitivity to parameter changes.</div>

          <div>Communication metrics</div>
          <div>Message volume, negotiation rounds, coordination overhead (ACL parsing time).</div>
        </div>

        <div class="refs">
          <h4>References</h4>
          <ul>
            <li>Macal, C.M. and North, M.J. (2010) ‘Tutorial on agent-based modelling and simulation’, <em>Journal of Simulation</em>, 4(3), pp. 151–162.</li>
          </ul>
        </div>
      </div>

      <div class="card prose">
        <h3>3) Small quantitative analysis: comparing two agent approaches</h3>
        <p>
          To demonstrate statistical reasoning, I outline how I would compare a baseline (e.g., rule-based or reactive agent)
          with an improved approach (e.g., hybrid/BDI or learning-enabled agent) using repeated runs.
        </p>

        <h4>Design</h4>
        <ul>
          <li>Run each agent policy multiple times (e.g., 20–50 runs) because outcomes vary under uncertainty.</li>
          <li>Record a consistent metric such as <strong>task success</strong> and <strong>time-to-complete</strong>.</li>
          <li>Report <strong>mean</strong>, <strong>standard deviation</strong>, and a <strong>confidence interval</strong>.</li>
        </ul>

        <h4>Example summary table (template)</h4>
        <div class="table-wrap" role="region" aria-label="Evaluation table" tabindex="0">
          <table>
            <thead>
              <tr>
                <th>Agent Type</th>
                <th>Runs (n)</th>
                <th>Success Rate</th>
                <th>Avg. Completion Time</th>
                <th>Notes</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>Baseline (reactive / rules)</td>
                <td>30</td>
                <td>0.78</td>
                <td>42.6 sec</td>
                <td>Fast decisions, weaker adaptation to unexpected states</td>
              </tr>
              <tr>
                <td>Improved (hybrid / deliberative)</td>
                <td>30</td>
                <td>0.88</td>
                <td>46.9 sec</td>
                <td>Higher success; slightly higher compute/latency</td>
              </tr>
            </tbody>
          </table>
        </div>

        <h4>Interpretation</h4>
        <p>
          This pattern (higher success but slightly higher completion time) is common when an agent adds deliberation,
          planning, or richer reasoning. The correct decision depends on operational constraints:
          if time is safety-critical, the baseline may still be preferred; if failure is costly, the improved agent may be justified.
        </p>

        <div class="refs">
          <h4>References</h4>
          <ul>
            <li>Wooldridge, M. (2021) <em>An Introduction to MultiAgent Systems</em>. 2nd edn. Chichester: John Wiley &amp; Sons.</li>
          </ul>
        </div>
      </div>

      <div class="card prose">
        <h3>4) Risk, bias and fairness: a statistical lens</h3>
        <p>
          Even when the overall performance looks strong, agent systems can perform unevenly across contexts or user groups.
          A basic statistical approach is to compute performance metrics across segments and compare error rates.
        </p>

        <h4>What to check</h4>
        <ul>
          <li><strong>Disparity in error rates</strong> (e.g., false positives/false negatives across groups).</li>
          <li><strong>Calibration</strong>: does confidence reflect true correctness?</li>
          <li><strong>Distribution shift</strong>: does performance degrade outside the training/assumed environment?</li>
        </ul>

        <h4>Why this matters for generative and agentic systems</h4>
        <p>
          Bias and fairness are not only ethical issues; they become measurable risks when scaled deployment amplifies harm.
          Governance should therefore include pre-deployment testing and continuous monitoring.
        </p>

        <div class="refs">
          <h4>References</h4>
          <ul>
            <li>Bolukbasi, T. et al. (2016) ‘Man is to computer programmer as woman is to homemaker? Debiasing word embeddings’, <em>Advances in Neural Information Processing Systems</em>, 29, pp. 4349–4357.</li>
            <li>Jobin, A., Ienca, M. and Vayena, E. (2019) ‘The global landscape of AI ethics guidelines’, <em>Nature Machine Intelligence</em>, 1(9), pp. 389–399.</li>
          </ul>
        </div>
      </div>

      <div class="card prose">
        <h3>5) Reliability and uncertainty reporting</h3>
        <p>
          Intelligent agents often produce outputs that appear confident, even when underlying certainty is low.
          A responsible statistical practice is to quantify uncertainty and define escalation rules.
        </p>

        <h4>Practical measures</h4>
        <ul>
          <li>Use <strong>confidence intervals</strong> around success rate or reward.</li>
          <li>Track <strong>variance</strong> across runs and identify unstable states.</li>
          <li>Define <strong>thresholds</strong> for human escalation when uncertainty is high.</li>
        </ul>

        <div class="refs">
          <h4>References</h4>
          <ul>
            <li>Shanahan, M. (2024) ‘Talking about large language models’, <em>Communications of the ACM</em>, 67(2), pp. 68–79.</li>
            <li>Floridi, L. et al. (2018) ‘AI4People—An ethical framework for a good AI society’, <em>Minds and Machines</em>, 28(4), pp. 689–707.</li>
          </ul>
        </div>
      </div>

      <div class="card prose">
        <h3>6) Mini conclusion</h3>
        <p>
          Statistics supports a disciplined view of intelligent agents: not only whether a system “works”, but how reliably,
          how fairly, and under what conditions it can be trusted. This aligns with the module’s emphasis on designing
          agent systems that are robust, explainable, and safe in real organisational environments.
        </p>
      </div>

    </section>

    <footer class="footer">
      <span>© Naeema Alnaqbi • Intelligent Agents e-Portfolio</span>
      <a class="toplink" href="#top">Back to top</a>
    </footer>

  </main>
</body>
</html>
