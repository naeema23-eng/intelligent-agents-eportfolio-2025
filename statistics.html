<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Statistics – Intelligent Agents e-Portfolio</title>
  <link rel="stylesheet" href="style.css" />
</head>

<body id="top">
  <a class="skip-link" href="#main">Skip to content</a>

  <header class="site-header">
    <div class="header-inner">
      <div class="brand">
        <h1>Intelligent Agents</h1>
        <div class="sub">MSc Artificial Intelligence – e-Portfolio</div>
      </div>

      <nav class="nav" aria-label="Main navigation">
        <a href="index.html">Home</a>
        <a href="discussion.html">Discussions</a>
        <a href="activities.html">Activities</a>
        <a class="active" href="statistics.html" aria-current="page">Statistics</a>
        <a href="reflections.html">Reflections</a>
        <a href="about.html">About</a>
      </nav>
    </div>
  </header>

  <main id="main" class="container">

    <section class="hero">
      <h2>Statistics</h2>
      <p>
        This page demonstrates quantitative thinking and evidence-based evaluation relevant to intelligent agents and
        multi-agent systems. It outlines how statistical reasoning supports performance evaluation, comparison,
        reliability assessment, and fairness analysis, and how these methods inform the evaluation of agent-based
        solutions discussed and developed throughout this module.
      </p>
      <div class="meta">
        <span class="badge">Evaluation</span>
        <span class="badge">Metrics</span>
        <span class="badge">Interpretation</span>
      </div>
    </section>

    <section class="grid">

      <div class="card prose">
        <h3>1) Why statistics matters for Intelligent Agents</h3>
        <p>
          Intelligent agents operate in uncertain environments and can produce variable outcomes depending on state,
          data, interactions, and stochastic policies. Statistics supports:
        </p>
        <ul>
          <li><strong>Performance evaluation</strong> (accuracy, reward, latency, success rate).</li>
          <li><strong>Reliability</strong> (variance, confidence intervals, stability across runs).</li>
          <li><strong>Comparison</strong> (baseline vs. improved agent policy; A/B-style evaluation).</li>
          <li><strong>Risk and fairness assessment</strong> (error rates across groups and contexts).</li>
        </ul>

        <div class="refs">
          <h4>References</h4>
          <ul>
            <li>Russell, S. and Norvig, P. (2021) <em>Artificial Intelligence: A Modern Approach</em>. 4th edn. Harlow: Pearson.</li>
            <li>Wooldridge, M. (2021) <em>An Introduction to MultiAgent Systems</em>. 2nd edn. Chichester: John Wiley &amp; Sons.</li>
          </ul>
        </div>
      </div>

      <div class="card prose">
        <h3>2) Example metrics for agent evaluation</h3>
        <p class="muted">
          The goal here is to show appropriate metrics and how they should be interpreted. Values are presented as
          example templates that illustrate suitable evaluation approaches.
        </p>

        <div class="kv">
          <div>Task outcome metrics</div>
          <div>Success rate, completion time, failure rate, throughput.</div>

          <div>Quality metrics</div>
          <div>Reward (mean cumulative), accuracy / F1 (if classification is involved), cost savings.</div>

          <div>Stability metrics</div>
          <div>Variance across runs, confidence intervals, sensitivity to parameter changes.</div>

          <div>Communication metrics</div>
          <div>Message volume, negotiation rounds, coordination overhead.</div>
        </div>

        <div class="refs">
          <h4>References</h4>
          <ul>
            <li>Macal, C.M. and North, M.J. (2010) ‘Tutorial on agent-based modelling and simulation’, <em>Journal of Simulation</em>, 4(3), pp. 151–162.</li>
          </ul>
        </div>
      </div>

      <div class="card prose">
        <h3>3) Small quantitative analysis: comparing two agent approaches</h3>
        <p>
          To demonstrate statistical reasoning, I outline how a baseline agent (e.g., reactive or rule-based) can be
          compared with an improved agent (e.g., hybrid or deliberative) using repeated experimental runs.
        </p>

        <h4>Design</h4>
        <ul>
          <li>Execute each agent policy multiple times because outcomes vary under uncertainty.</li>
          <li>Record consistent metrics such as task success and time-to-complete.</li>
          <li>Report mean performance, standard deviation, and confidence intervals.</li>
        </ul>

        <h4>Example summary table</h4>
        <div class="table-wrap" role="region" aria-label="Evaluation table" tabindex="0">
          <table>
            <thead>
              <tr>
                <th>Agent Type</th>
                <th>Runs (n)</th>
                <th>Success Rate</th>
                <th>Avg. Completion Time</th>
                <th>Notes</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>Baseline (reactive / rules)</td>
                <td>30</td>
                <td>0.78</td>
                <td>42.6 sec</td>
                <td>Fast responses, limited adaptation</td>
              </tr>
              <tr>
                <td>Improved (hybrid / deliberative)</td>
                <td>30</td>
                <td>0.88</td>
                <td>46.9 sec</td>
                <td>Higher success, increased reasoning cost</td>
              </tr>
            </tbody>
          </table>
        </div>

        <h4>Interpretation</h4>
        <p>
          The improved agent achieves a higher success rate at the cost of increased computation time. Whether this trade-off
          is acceptable depends on operational constraints, such as time sensitivity and the cost of failure.
        </p>

        <div class="refs">
          <h4>References</h4>
          <ul>
            <li>Wooldridge, M. (2021) <em>An Introduction to MultiAgent Systems</em>. 2nd edn. Chichester: John Wiley &amp; Sons.</li>
          </ul>
        </div>
      </div>

      <div class="card prose">
        <h3>Link to module activities and projects</h3>
        <p>
          The statistical approaches presented on this page informed how I evaluated and reflected on my work throughout
          the module. In the collaborative discussions, statistical reasoning supported comparison between alternative
          agent architectures and coordination strategies. In the Unit 6 development project, similar evaluation
          principles guided decisions prioritising determinism, auditability, and reliability over more complex but
          harder-to-verify agent interactions.
        </p>

        <p>
          Although the module did not require large-scale experimental datasets, this framework demonstrates how I would
          design, measure, and interpret agent performance in real organisational or safety-critical deployments.
        </p>
      </div>

      <div class="card prose">
        <h3>4) Risk, bias and fairness: a statistical lens</h3>
        <p>
          Even when overall performance appears strong, agent systems can perform unevenly across contexts or user groups.
          Statistical analysis supports identification of these disparities.
        </p>

        <ul>
          <li><strong>Disparity in error rates</strong> across groups or scenarios.</li>
          <li><strong>Calibration</strong> between predicted confidence and actual outcomes.</li>
          <li><strong>Distribution shift</strong> effects when operating outside assumed environments.</li>
        </ul>

        <p>
          Bias and fairness therefore become measurable risks rather than abstract ethical concerns, reinforcing the need
          for governance, testing, and monitoring.
        </p>

        <div class="refs">
          <h4>References</h4>
          <ul>
            <li>Bolukbasi, T. et al. (2016) ‘Debiasing word embeddings’, <em>Advances in Neural Information Processing Systems</em>, 29, pp. 4349–4357.</li>
            <li>Jobin, A., Ienca, M. and Vayena, E. (2019) ‘The global landscape of AI ethics guidelines’, <em>Nature Machine Intelligence</em>, 1(9), pp. 389–399.</li>
          </ul>
        </div>
      </div>

      <div class="card prose">
        <h3>5) Reliability and uncertainty reporting</h3>
        <p>
          Intelligent agents may generate confident outputs even when uncertainty is high. Responsible statistical
          practice requires explicit uncertainty reporting and escalation strategies.
        </p>

        <ul>
          <li>Confidence intervals around success rate or reward.</li>
          <li>Variance tracking across repeated runs.</li>
          <li>Defined thresholds for human escalation under high uncertainty.</li>
        </ul>

        <div class="refs">
          <h4>References</h4>
          <ul>
            <li>Shanahan, M. (2024) ‘Talking about large language models’, <em>Communications of the ACM</em>, 67(2), pp. 68–79.</li>
            <li>Floridi, L. et al. (2018) ‘AI4People—An ethical framework for a good AI society’, <em>Minds and Machines</em>, 28(4), pp. 689–707.</li>
          </ul>
        </div>
      </div>

      <div class="card prose">
        <h3>6) Mini conclusion</h3>
        <p>
          Statistics provides a disciplined foundation for evaluating intelligent agents, extending beyond whether a
          system functions to how reliably, fairly, and safely it operates. This aligns with the module’s emphasis on
          designing agent systems that are robust, explainable, and trustworthy in real-world contexts.
        </p>
      </div>

    </section>

    <footer class="footer">
      <span>© Naeema Alnaqbi • Intelligent Agents e-Portfolio</span>
      <a class="toplink" href="#top">Back to top</a>
    </footer>

  </main>
</body>
</html>


