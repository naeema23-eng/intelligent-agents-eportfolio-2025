<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Discussions – Intelligent Agents e-Portfolio</title>
  <link rel="stylesheet" href="style.css" />
</head>

<body id="top">
  <a class="skip-link" href="#main">Skip to content</a>

  <header class="site-header">
    <div class="header-inner">
      <div class="brand">
        <h1>Intelligent Agents</h1>
        <div class="sub">MSc Artificial Intelligence – e-Portfolio</div>
      </div>

      <nav class="nav" aria-label="Main navigation">
        <a href="index.html">Home</a>
        <a class="active" href="discussion.html" aria-current="page">Discussions</a>
        <a href="activities.html">Activities</a>
        <a href="statistics.html">Statistics</a>
        <a href="reflections.html">Reflections</a>
        <a href="about.html">About</a>
      </nav>
    </div>
  </header>

  <main id="main" class="container">
    <section class="hero">
      <h2>Collaborative Discussions</h2>
      <p>
        This page presents my contributions to the module discussions, including initial posts, peer responses, and summary posts.
        Content is organised by discussion topic for clarity and assessment readiness.
      </p>
      <div class="meta">
        <span class="badge">Module: Intelligent Agents</span>
        <span class="badge">Format: Initial post + peer responses + summary post</span>
      </div>
    </section>

    <h2 class="section-title">Discussion 1</h2>
    <div class="card">
      <h3>The Rise and Benefits of Agent-Based Systems in Modern Organisations</h3>

      <details open>
        <summary>
          <span>My Initial Post</span>
          <span class="summary-right">16 Nov 2025 • 5:30 PM</span>
        </summary>
        <div class="detail-body prose">
          <div class="kv">
            <div>Author</div><div>Naeema Alnaqbi</div>
            <div>Type</div><div>Initial Post</div>
          </div>

          <p><strong>The Rise and Benefits of Agent-Based Systems in Modern Organisations</strong></p>

          <p>
            The rise of agent-based systems (ABS) reflects a shift from centralised architectures toward decentralised,
            adaptive intelligence that can cope with the complexity of modern organisations. Traditional systems struggle
            when environments are distributed, dynamic and heterogeneous, whereas ABS model such settings as collections
            of autonomous agents that perceive, reason and act locally while still contributing to global organisational
            goals (Wooldridge and Jennings, 1995; Wooldridge, 2021). Advances in artificial intelligence, especially in
            knowledge representation and learning, combined with the growth of distributed computing, have made these
            systems feasible at scale (Russell and Norvig, 2021; Macal and North, 2010).
          </p>

          <p>
            From a technical perspective, several agent models support different organisational needs. Logic-based
            Belief–Desire–Intention (BDI) agents link first-order logic and practical reasoning to operational
            decision-making in complex domains (Rao and Georgeff, 1995). Behaviour-based agents, inspired by Brooks’
            “intelligence without representation”, provide fast, reactive control in robotics and embedded systems
            (Brooks, 1991). Modern hybrid architectures combine deliberative and reactive layers, while multi-agent
            reinforcement learning extends agents with adaptive behaviour in cooperative and competitive environments
            (Lowe et al., 2017; Winikoff, 2017).
          </p>

          <p>
            The organisational benefits of ABS are visible in several real-world systems. In manufacturing,
            DaimlerChrysler’s agent-based control improved robustness and flexibility on production lines operating under
            uncertainty (Jennings and Bussmann, 2003). In logistics, Kiva Systems (now Amazon Robotics) used hundreds of
            warehouse robots coordinated through an agent-based approach to double picker productivity and increase
            throughput (Wurman, D’Andrea and Mountz, 2008). At infrastructure level, the JADE framework provides a
            FIPA-compliant platform that allows organisations to deploy interoperable agents for coordination,
            negotiation and distributed control (Bellifemine, Caire and Greenwood, 2007). Recent multi-agent
            reinforcement learning work, such as DeepMind-style actor–critic methods, demonstrates how agents can jointly
            learn strategies for complex tasks, pointing to future applications in markets, traffic and resource
            allocation (Lowe et al., 2017).
          </p>

          <p>
            However, these benefits come with non-trivial risks. Autonomous agents can produce emergent behaviours that
            conflict with organisational policies or legal requirements if they are not properly governed (Dignum, 2019;
            Rahwan, 2018). In data-driven agents, privacy and compliance with frameworks such as the EU General Data
            Protection Regulation (GDPR) must be considered when agents share or process personal data across distributed
            environments (Voigt and Von dem Bussche, 2017). This makes accountability, formal verification and transparent
            decision-making essential (Fisher et al., 2021; Raji et al., 2020).
          </p>

          <p>
            Overall, agent-based systems are well aligned with organisations that operate in distributed, uncertain and
            fast-changing contexts. When combined with clear governance, human oversight and legal safeguards, they offer
            a powerful way to scale decision-making, improve resilience and manage complexity.
          </p>

          <div class="refs">
            <h4>References</h4>
            <ul>
              <li>Bellifemine, F., Caire, G. and Greenwood, D. (2007) <em>Developing Multi-Agent Systems with JADE</em>. Chichester: John Wiley &amp; Sons.</li>
              <li>Brooks, R.A. (1991) ‘Intelligence without representation’, <em>Artificial Intelligence</em>, 47(1–3), pp. 139–159.</li>
              <li>Dignum, V. (2019) <em>Responsible Artificial Intelligence: How to Develop and Use AI in a Responsible Way</em>. Cham: Springer.</li>
              <li>Fisher, M. et al. (2021) ‘Towards a framework for certification of reliable autonomous systems’, <em>Autonomous Agents and Multi-Agent Systems</em>, 35(1), pp. 1–65.</li>
              <li>Jennings, N.R. and Bussmann, S. (2003) ‘Agent-based control systems: Why are they suited to engineering complex systems?’, <em>IEEE Control Systems Magazine</em>, 23(3), pp. 61–73.</li>
              <li>Lowe, R. et al. (2017) ‘Multi-agent actor-critic for mixed cooperative–competitive environments’, in <em>NeurIPS 2017</em>, pp. 6379–6390.</li>
              <li>Macal, C.M. and North, M.J. (2010) ‘Tutorial on agent-based modelling and simulation’, <em>Journal of Simulation</em>, 4(3), pp. 151–162.</li>
              <li>Raji, I.D., Smart, A. and White, R.N. (2020) ‘Closing the AI accountability gap’, in <em>FAccT 2020</em>, pp. 336–345.</li>
              <li>Rao, A.S. and Georgeff, M.P. (1995) ‘BDI agents: From theory to practice’, in <em>ICMAS 1995</em>, pp. 312–319.</li>
              <li>Rahwan, I. (2018) ‘Society-in-the-loop: Programming the algorithmic social contract’, <em>Ethics and Information Technology</em>, 20(1), pp. 5–14.</li>
              <li>Russell, S. and Norvig, P. (2021) <em>Artificial Intelligence: A Modern Approach</em>. 4th edn. Harlow: Pearson.</li>
              <li>Voigt, P. and Von dem Bussche, A. (2017) <em>The EU General Data Protection Regulation (GDPR): A Practical Guide</em>. Cham: Springer.</li>
              <li>Winikoff, M. (2017) ‘Challenges and directions for engineering multi-agent systems’, <em>Autonomous Agents and Multi-Agent Systems</em>, 31(4), pp. 779–817.</li>
              <li>Wooldridge, M. (2021) <em>An Introduction to MultiAgent Systems</em>. 2nd edn. Chichester: John Wiley &amp; Sons.</li>
              <li>Wooldridge, M. and Jennings, N.R. (1995) ‘Intelligent agents: Theory and practice’, <em>The Knowledge Engineering Review</em>, 10(2), pp. 115–152.</li>
              <li>Wurman, P.R., D’Andrea, R. and Mountz, M. (2008) ‘Coordinating hundreds of cooperative robots for warehouse automation’, <em>AI Magazine</em>, 29(1), pp. 9–19.</li>
            </ul>
          </div>
        </div>
      </details>

      <details>
        <summary>
          <span>Peer Reply to Me (Yousif Ali Karam Yousif Almaazmi)</span>
          <span class="summary-right">25 Nov 2025 • 12:58 PM</span>
        </summary>
        <div class="detail-body prose">
          <div class="kv">
            <div>From</div><div>Yousif Ali Karam Yousif Almaazmi</div>
            <div>To</div><div>Naeema Alnaqbi</div>
            <div>Type</div><div>Reply to my initial post</div>
          </div>

          <p>
            Naeema, your analysis on the increasing and positive aspects of agent-based system (ABS), especially on the
            use in contemporary organisations, is quite comprehensive. I concur that the centralised systems are
            re-centralising to more decentralised and responsive intelligence systems that will be crucial in dealing
            with the complexity of the modern environment. The ability of ABS to take dynamic, distributed, and
            heterogeneous environments, of course, is a strong argument in their favour in organisations that are
            interested in enhancing decision-making procedures under dynamic and unpredictable conditions (Ponta et al.,
            2024). The fact that you refer to other models of agents, including BDI and behaviour-based agents, is an
            effective way to focus attention on the fact that ABS is both flexible and can be used to meet numerous
            organisational requirements. The examples that you have given, such as those of DaimlerChrysler and Amazon
            Robotics, demonstrate the practical gains that ABS will provide in such aspects as production and logistics.
            This information is evident in these real-life examples that depict how ABS can induce efficiencies and create
            better operational performance (McDonald &amp; Osgood, 2023).
          </p>

          <p>
            But as you have mentioned, you cannot ignore the risks attributed to ABS, especially concerning emergent
            behaviours, governance, privacy, and compliance. Considering that leveraging the autonomy of agents can result
            in innovation, the aspect of accountability and the possibility of agents performing their duties in manners
            that do not comply with organisational or legal demands present crucial questions (Riazi et al., 2024). Issues
            of transparency and proper governance structures of these systems are very demanding, particularly when it
            comes to data privacy issues such as those which are generated by GDPR. To conclude, ABS are quite beneficial,
            yet their governance and ethical aspects should be taken into serious consideration (Ponta et al., 2024).
            Organisations have to find a balance between utilising the advantages of autonomy whilst making sure that such
            systems are used responsibly and in compliance.
          </p>

          <div class="refs">
            <h4>References (peer)</h4>
            <ul>
              <li>McDonald, G.W. and Osgood, N.D. (2023) ‘Agent-based modeling and its trade-offs’, in <em>Mathematics of Public Health</em>, pp. 209–242. Springer. https://doi.org/10.1007/978-3-031-40805-2_9</li>
              <li>Ponta, L. et al. (2024) ‘Reacting and recovering after an innovation failure. An agent-based approach’, <em>Technovation</em>, 129, 102884. https://doi.org/10.1016/j.technovation.2023.102884</li>
              <li>Riazi, M. et al. (2024) ‘Design, simulation and feasibility…’, <em>Scientific Reports</em>, 14(1), 23182. https://doi.org/10.1038/s41598-024-74519-w</li>
            </ul>
          </div>
        </div>
      </details>

      <details>
        <summary>
          <span>My Peer Response (to Ariel Mella)</span>
          <span class="summary-right">16 Nov 2025 • 5:43 PM</span>
        </summary>
        <div class="detail-body prose">
          <div class="kv">
            <div>To</div><div>Ariel Mella</div>
            <div>Type</div><div>Peer Response</div>
          </div>

          <p>
            Hi Ariel,
          </p>
          <p>
            Your post offers a rich, historically grounded view of how agent-based systems (ABS) emerged from centralised
            ERP-style architectures to today’s microservices, cloud and service-oriented designs. I especially liked how
            you connected FIPA-compliant platforms with concrete industrial cases such as DaimlerChrysler, Kiva and
            Alibaba, which clearly demonstrate that ABS deliver measurable value rather than remaining only theoretical
            (Bellifemine, Poggi and Rimassa, 2001; Jennings and Bussmann, 2003; Wurman, D’Andrea and Mountz, 2008).
          </p>

          <p>
            I agree with your emphasis on scalability and resilience, but I see your examples also highlighting a second
            layer of value: learning and coordination under uncertainty. Your reference to MADDPG aligns well with current
            work in multi-agent reinforcement learning, where centralised training with decentralised execution helps
            agents learn stable joint policies in non-stationary environments (Lowe et al., 2017). This addresses some of
            the “credit assignment” and coordination problems you mentioned, and connects directly to industrial settings
            such as traffic optimisation, RTB markets and large-scale logistics (Foerster et al., 2018).
          </p>

          <p>
            At the same time, your discussion of technical debt and process misalignment is crucial. As you note with
            Sculley et al. (2015), simply “dropping in” learning agents into legacy workflows can create fragile systems
            that are hard to monitor and debug. Dignum (2019) and Rahwan (2018) argue that such systems require explicit
            governance frameworks that make responsibilities, oversight and escalation paths clear. I would add that, when
            ABS operate on personal or behavioural data—as in marketing and personalised recommender settings—they must
            also comply with privacy and data-protection requirements such as GDPR, which constrains how agents log, share
            and retain information (Voigt and Von dem Bussche, 2017).
          </p>

          <p>
            Overall, your post helped me see ABS less as isolated technical components and more as socio-technical systems
            that interact with processes, regulations and organisational culture. Your combination of architectures,
            real-world case studies and critical reflection on risk provides an excellent foundation for thinking about
            how organisations should adopt ABS responsibly.
          </p>

          <div class="refs">
            <h4>References</h4>
            <ul>
              <li>Bellifemine, F., Poggi, A. and Rimassa, G. (2001) ‘Developing multi agent systems with a FIPA compliant agent framework’, <em>Software: Practice and Experience</em>, 31(2), pp. 103–128.</li>
              <li>Dignum, V. (2019) <em>Responsible Artificial Intelligence</em>. Cham: Springer.</li>
              <li>Foerster, J.N. et al. (2018) ‘Counterfactual multi-agent policy gradients’, <em>AAAI</em>, 32(1).</li>
              <li>Jennings, N.R. and Bussmann, S. (2003) ‘Agent-based control systems…’, <em>IEEE Control Systems Magazine</em>, 23(3), pp. 61–73.</li>
              <li>Lowe, R. et al. (2017) ‘Multi-agent actor-critic…’, <em>NeurIPS 2017</em>, pp. 6379–6390.</li>
              <li>Rahwan, I. (2018) ‘Society-in-the-loop…’, <em>Ethics and Information Technology</em>, 20(1), pp. 5–14.</li>
              <li>Sculley, D. et al. (2015) ‘Hidden technical debt…’, <em>NeurIPS 2015</em>, 28, pp. 2503–2511.</li>
              <li>Voigt, P. and Von dem Bussche, A. (2017) <em>GDPR: A Practical Guide</em>. Cham: Springer.</li>
              <li>Wurman, P.R., D’Andrea, R. and Mountz, M. (2008) ‘Coordinating hundreds…’, <em>AI Magazine</em>, 29(1), pp. 9–19.</li>
            </ul>
          </div>
        </div>
      </details>

      <details>
        <summary>
          <span>My Peer Response (to Bashair Alhosani)</span>
          <span class="summary-right">16 Nov 2025 • 5:36 PM</span>
        </summary>
        <div class="detail-body prose">
          <div class="kv">
            <div>To</div><div>Bashair Alhosani</div>
            <div>Type</div><div>Peer Response</div>
          </div>

          <p>Hi Bashair,</p>

          <p>
            Your post gives a clear and well-structured explanation of how agent-based systems (ABS) emerged from the
            limitations of centralised architectures and why they are attractive for modern organisations. I particularly
            appreciated your use of the BDI model and the way you linked it back to AI foundations, showing how beliefs,
            desires and intentions provide a bridge between logical reasoning and practical decision-making (Rao and
            Georgeff, 1995; Russell and Norvig, 2021).
          </p>

          <p>
            Building on your discussion of modularity and real-time responsiveness, I think your examples naturally point
            towards governance and verification as key success factors. As you suggested, the same flexibility that allows
            agents to adapt locally can introduce risks of misalignment or unexpected emergent behaviour. Winikoff (2017)
            and Fisher et al. (2021) argue that formal verification and certification frameworks are increasingly
            necessary, especially when ABS support safety-critical or high-impact decisions. These ideas connect strongly
            with your point about aligning agent behaviour with organisational policies.
          </p>

          <p>
            I also found your reference to hybrid architectures very relevant. In practice, many large-scale systems
            combine reactive agents for low-latency responses with deliberative or learning-based components for
            higher-level planning (Wooldridge, 2021; Bellifemine, Caire and Greenwood, 2007). Kiva’s warehouse robots and
            similar systems show how this layered approach can deliver both robustness and adaptability in real deployments
            (Wurman, D’Andrea and Mountz, 2008).
          </p>

          <p>
            Finally, I appreciate that you acknowledged ethical and security concerns. When agents operate on distributed
            organisational data—especially personal or sensitive information—legal frameworks such as GDPR impose
            constraints on how data can be processed, logged and shared (Voigt and Von dem Bussche, 2017). Dignum (2019)
            and Rahwan (2018) emphasise that ABS should therefore be embedded within broader responsible-AI and
            “society-in-the-loop” governance structures. Your argument about balancing autonomy with control fits very well
            within this perspective.
          </p>

          <p>
            Overall, your post provides a strong foundation that connects theory, architecture and organisational practice.
            By bringing in verification, governance and legal considerations, it becomes a very complete view of what it
            takes to deploy ABS safely and effectively.
          </p>

          <div class="refs">
            <h4>References</h4>
            <ul>
              <li>Bellifemine, F., Caire, G. and Greenwood, D. (2007) <em>Developing Multi-Agent Systems with JADE</em>. Chichester: John Wiley &amp; Sons.</li>
              <li>Dignum, V. (2019) <em>Responsible Artificial Intelligence</em>. Cham: Springer.</li>
              <li>Fisher, M. et al. (2021) ‘Towards a framework for certification of reliable autonomous systems’, <em>Autonomous Agents and Multi-Agent Systems</em>, 35(1), pp. 1–65.</li>
              <li>Rao, A.S. and Georgeff, M.P. (1995) ‘BDI agents: From theory to practice’, in <em>ICMAS 1995</em>, pp. 312–319.</li>
              <li>Rahwan, I. (2018) ‘Society-in-the-loop…’, <em>Ethics and Information Technology</em>, 20(1), pp. 5–14.</li>
              <li>Russell, S. and Norvig, P. (2021) <em>Artificial Intelligence: A Modern Approach</em>. 4th edn. Harlow: Pearson.</li>
              <li>Voigt, P. and Von dem Bussche, A. (2017) <em>GDPR: A Practical Guide</em>. Cham: Springer.</li>
              <li>Winikoff, M. (2017) ‘Challenges and directions…’, <em>Autonomous Agents and Multi-Agent Systems</em>, 31(4), pp. 779–817.</li>
              <li>Wooldridge, M. (2021) <em>An Introduction to MultiAgent Systems</em>. 2nd edn. Chichester: John Wiley &amp; Sons.</li>
              <li>Wurman, P.R., D’Andrea, R. and Mountz, M. (2008) ‘Coordinating hundreds…’, <em>AI Magazine</em>, 29(1), pp. 9–19.</li>
            </ul>
          </div>
        </div>
      </details>

      <details>
        <summary>
          <span>My Summary Post</span>
          <span class="summary-right">16 Nov 2025 • 5:48 PM</span>
        </summary>
        <div class="detail-body prose">
          <div class="kv">
            <div>Author</div><div>Naeema Alnaqbi</div>
            <div>Type</div><div>Summary Post</div>
          </div>

          <p><strong>Agent-Based Systems and Decentralised Intelligence</strong></p>

          <p>
            Across this discussion, my understanding of agent-based systems (ABS) has evolved from seeing them as a
            technical alternative to centralised architectures to recognising them as a broader socio-technical approach
            to managing complexity. The units introduced core concepts such as intelligent agents, first-order logic and
            agent architectures, while peers’ posts illustrated how these ideas translate into real systems in logistics,
            healthcare, finance and manufacturing (Wooldridge and Jennings, 1995; Macal and North, 2010; Wooldridge, 2021).
          </p>

          <p>
            A consistent theme was that ABS are well suited to complex, distributed and dynamic environments. From a
            modelling perspective, agent-based simulation allows organisations to explore “what-if” scenarios and emergent
            behaviour before changing real operations (Macal and North, 2010). Operationally, examples such as
            DaimlerChrysler’s production lines and Kiva’s warehouse robots show that distributed agents can increase
            robustness, throughput and adaptability compared to rigid centralised control (Jennings and Bussmann, 2003;
            Wurman, D’Andrea and Mountz, 2008). Frameworks like JADE and FIPA standards provide practical infrastructure
            for building interoperable multi-agent platforms across heterogeneous systems (Bellifemine, Poggi and Rimassa,
            2001; Bellifemine, Caire and Greenwood, 2007).
          </p>

          <p>
            The course content on BDI, reactive and hybrid architectures helped me understand how agents link knowledge,
            reasoning and action. BDI agents implement the Belief–Desire–Intention model derived from practical reasoning,
            while behaviour-based agents provide fast responses in dynamic environments (Rao and Georgeff, 1995; Brooks,
            1991; Russell and Norvig, 2021). Peers extended this by highlighting multi-agent reinforcement learning methods
            such as MADDPG, where agents learn coordinated policies under centralised training but act decentrally at run
            time (Lowe et al., 2017; Foerster et al., 2018).
          </p>

          <p>
            However, peer responses also shifted the discussion from benefits to risks and governance. Contributors
            emphasised that autonomy and emergence can lead to incidents such as flash crashes, unfair decisions or
            misaligned incentives if agent goals diverge from organisational or societal objectives (Dignum, 2019; Rahwan,
            2018). This aligns with work on responsible AI and “society-in-the-loop”, which argues for explicit oversight,
            auditability and alignment mechanisms for autonomous systems (Raji, Smart and White, 2020). Formal verification
            and certification frameworks were discussed as ways to bound behaviour and increase trust, particularly in
            safety-critical contexts (Fisher et al., 2021; Winikoff, 2017).
          </p>

          <p>
            An additional dimension is the legal and ethical context. Many real-world ABS operate on personal, behavioural
            or transactional data. In such cases, regulations like the EU GDPR impose requirements on data minimisation,
            transparency and accountability, which must be reflected in agent design, logging and communication (Voigt and
            Von dem Bussche, 2017). This reinforced my view that deploying ABS is not only a system-design decision but
            also a governance and compliance challenge.
          </p>

          <p>
            Overall, the discussion helped me integrate theory, applications and critical reflection. Agent-based systems
            appear most valuable when organisations combine decentralised decision-making with: (1) explicit representations
            of goals and knowledge, (2) redesigned processes that account for autonomy, (3) human-in-the-loop oversight and
            (4) strong legal, ethical and verification frameworks. When these elements are present, ABS provide a powerful,
            future-oriented approach for operating in complex digital environments.
          </p>

          <div class="refs">
            <h4>References</h4>
            <ul>
              <li>Bellifemine, F., Caire, G. and Greenwood, D. (2007) <em>Developing Multi-Agent Systems with JADE</em>. Chichester: John Wiley &amp; Sons.</li>
              <li>Bellifemine, F., Poggi, A. and Rimassa, G. (2001) ‘Developing multi agent systems…’, <em>Software: Practice and Experience</em>, 31(2), pp. 103–128.</li>
              <li>Brooks, R.A. (1991) ‘Intelligence without representation’, <em>Artificial Intelligence</em>, 47(1–3), pp. 139–159.</li>
              <li>Dignum, V. (2019) <em>Responsible Artificial Intelligence</em>. Cham: Springer.</li>
              <li>Fisher, M. et al. (2021) ‘Towards a framework for certification…’, <em>Autonomous Agents and Multi-Agent Systems</em>, 35(1), pp. 1–65.</li>
              <li>Foerster, J.N. et al. (2018) ‘Counterfactual multi-agent policy gradients’, <em>AAAI</em>, 32(1).</li>
              <li>Jennings, N.R. and Bussmann, S. (2003) ‘Agent-based control systems…’, <em>IEEE Control Systems Magazine</em>, 23(3), pp. 61–73.</li>
              <li>Lowe, R. et al. (2017) ‘Multi-agent actor-critic…’, <em>NeurIPS 2017</em>, pp. 6379–6390.</li>
              <li>Macal, C.M. and North, M.J. (2010) ‘Tutorial on agent-based modelling…’, <em>Journal of Simulation</em>, 4(3), pp. 151–162.</li>
              <li>Raji, I.D., Smart, A. and White, R.N. (2020) ‘Closing the AI accountability gap’, in <em>FAccT 2020</em>, pp. 336–345.</li>
              <li>Rao, A.S. and Georgeff, M.P. (1995) ‘BDI agents…’, in <em>ICMAS 1995</em>, pp. 312–319.</li>
              <li>Rahwan, I. (2018) ‘Society-in-the-loop…’, <em>Ethics and Information Technology</em>, 20(1), pp. 5–14.</li>
              <li>Russell, S. and Norvig, P. (2021) <em>Artificial Intelligence: A Modern Approach</em>. 4th edn. Harlow: Pearson.</li>
              <li>Voigt, P. and Von dem Bussche, A. (2017) <em>GDPR: A Practical Guide</em>. Cham: Springer.</li>
              <li>Winikoff, M. (2017) ‘Challenges and directions…’, <em>Autonomous Agents and Multi-Agent Systems</em>, 31(4), pp. 779–817.</li>
              <li>Wooldridge, M. (2021) <em>An Introduction to MultiAgent Systems</em>. 2nd edn. Chichester: John Wiley &amp; Sons.</li>
              <li>Wooldridge, M. and Jennings, N.R. (1995) ‘Intelligent agents: Theory and practice’, <em>The Knowledge Engineering Review</em>, 10(2), pp. 115–152.</li>
              <li>Wurman, P.R., D’Andrea, R. and Mountz, M. (2008) ‘Coordinating hundreds…’, <em>AI Magazine</em>, 29(1), pp. 9–19.</li>
            </ul>
          </div>
        </div>
      </details>
    </div>

    <hr class="sep" />

    <h2 class="section-title">Discussion 2</h2>
    <div class="card">
      <h3>Agent Communication Languages (ACLs) such as KQML</h3>

      <details open>
        <summary>
          <span>My Initial Post</span>
          <span class="summary-right">08 Dec 2025 • 7:35 AM</span>
        </summary>
        <div class="detail-body prose">
          <div class="kv">
            <div>Author</div><div>Naeema Alnaqbi</div>
            <div>Type</div><div>Initial Post</div>
          </div>

          <p>
            Agent Communication Languages (ACLs) such as the Knowledge Query and Manipulation Language (KQML) were designed
            to enable agents to exchange not only data but also communicative intent. Their main advantage lies in
            separating what is communicated from why it is communicated. Through performatives such as ask, tell or
            achieve, ACLs allow agents to negotiate, plan and coordinate actions at a knowledge level rather than simply
            transmitting information (Finin et al., 1994). This aligns closely with foundational agent models where
            behaviour is guided by beliefs, goals and intentions.
          </p>

          <p>
            A major strength of ACLs is their suitability for heterogeneous, distributed environments. Because KQML messages
            follow standard semantics, agents written in different programming languages or deployed across different
            machines can still collaborate effectively (Labrou and Finin, 1997). For example, in emergency medical response
            systems, autonomous agents may need to negotiate resource allocation or communicate levels of clinical
            uncertainty—interactions that cannot be represented through simple Python or Java method calls but fit
            naturally within KQML’s communicative acts.
          </p>

          <p>
            Despite their benefits, ACLs introduce notable challenges. They require parsing, ontology alignment and semantic
            interpretation, which increase processing overhead. Ambiguous or inconsistent ontologies may lead agents to
            interpret messages differently, and such misunderstandings often emerge only at runtime (Chaib-Draa and Dignum,
            2002). By contrast, method invocation in Python or Java provides faster, tightly coupled, and more predictable
            execution because interfaces are clearly defined and errors are detected earlier (Weiss, 1999).
          </p>

          <p>
            Method invocation is therefore ideal for homogenous, structured systems but lacks the expressive power required
            for autonomous negotiation or distributed reasoning. As multi-agent systems increasingly incorporate learning
            and decision-making capabilities, the need for flexible, semantically grounded communication—beyond method
            invocation—continues to grow (Wooldridge, 2009).
          </p>

          <div class="refs">
            <h4>References</h4>
            <ul>
              <li>Chaib-Draa, B. and Dignum, F. (2002) ‘Trends in agent communication language’, <em>Computational Intelligence</em>, 18(2), pp. 89–101.</li>
              <li>Finin, T., Fritzson, R., McKay, D. and McEntire, R. (1994) ‘KQML as an agent communication language’, in <em>CIKM</em>. New York: ACM, pp. 456–463.</li>
              <li>Labrou, Y. and Finin, T. (1997) <em>A proposal for a new KQML specification</em>. Technical Report TR CS-97-03. Baltimore: UMBC.</li>
              <li>Weiss, G. (1999) <em>Multiagent systems: A modern approach to distributed artificial intelligence</em>. Cambridge, MA: MIT Press.</li>
              <li>Wooldridge, M. (2009) <em>An introduction to multiagent systems</em>. 2nd edn. Chichester: Wiley.</li>
            </ul>
          </div>
        </div>
      </details>

      <details>
        <summary>
          <span>My Reply to Ariel Mella</span>
          <span class="summary-right">08 Dec 2025 • 7:51 AM</span>
        </summary>
        <div class="detail-body prose">
          <div class="kv">
            <div>To</div><div>Ariel Mella</div>
            <div>Type</div><div>Reply</div>
          </div>

          <p>Hello Ariel,</p>

          <p>
            Your post presents a strong conceptual comparison between ACLs and method invocation, especially your emphasis
            on how ACLs separate intent from implementation. This distinction is central to the progression from procedural
            communication to knowledge-level communication in multi-agent systems (Finin, Labrou and Mayfield, 1994). Your
            link to BDI agents is also insightful. BDI architectures depend heavily on interpretive and deliberative
            capabilities, making ACLs a natural fit for goal-driven reasoning (Rao and Georgeff, 1995; Wooldridge, 2009).
          </p>

          <p>
            One point you highlighted—semantic overhead—is significant, and it becomes even more challenging when agents
            use evolving or incomplete ontologies. A preventive measure here is the adoption of shared, version-controlled
            domain ontologies, as recommended in early ontology engineering work (Gruber, 1993). This helps ensure that
            communicative acts have consistent meaning across all agents. Another strategy is implementing FIPA-compliant
            interaction protocols, which define structured sequences for negotiation or information exchange, reducing
            miscommunication and improving predictability in the system.
          </p>

          <p>
            Your comparison to method invocation is accurate: traditional calls enforce rigid control flow, eliminate
            uncertainty, and reduce processing overhead. However, as Tsochev, Trifonov and Naydenov (2015) noted, this
            simplicity becomes a constraint when agents require independence or asynchronous behaviour. One useful extension
            would be considering hybrid approaches, where method invocation is used for internal reasoning and ACLs only for
            inter-agent exchanges.
          </p>

          <p>
            Overall, your contribution demonstrates a deep understanding of contemporary multi-agent communication
            challenges and the architectural trade-offs involved.
          </p>

          <div class="refs">
            <h4>References</h4>
            <ul>
              <li>Finin, T., Labrou, Y. and Mayfield, J. (1994) ‘KQML as an agent communication language’, in <em>CIKM</em>. New York: ACM.</li>
              <li>Gruber, T. (1993) ‘A translation approach to portable ontology specifications’, <em>Knowledge Acquisition</em>, 5(2), pp. 199–220.</li>
              <li>Rao, A.S. and Georgeff, M.P. (1995) ‘BDI agents: From theory to practice’, in <em>ICMAS’95</em>.</li>
              <li>Tsochev, G., Trifonov, R. and Naydenov, G. (2015) ‘Agent communication languages comparison: FIPA-ACL and KQML’, in <em>Computer Science 2015</em>.</li>
              <li>Wooldridge, M. (2009) <em>An Introduction to Multi-Agent Systems</em>. 2nd edn. Chichester: Wiley.</li>
            </ul>
          </div>
        </div>
      </details>

      <details>
        <summary>
          <span>My Peer Response (to Yousif Ali Karam Yousif Almaazmi)</span>
          <span class="summary-right">08 Dec 2025 • 7:46 AM</span>
        </summary>
        <div class="detail-body prose">
          <div class="kv">
            <div>To</div><div>Yousif Ali Karam Yousif Almaazmi</div>
            <div>Type</div><div>Peer Response</div>
          </div>

          <p>Hello Yousif,</p>

          <p>
            Your post provides a clear and well-rounded explanation of the semantic richness that ACLs bring to agent
            interaction. Your description of performatives such as ask, tell and achieve captures exactly why ACLs support
            cooperation and negotiation more effectively than method invocation (Qian et al., 2023). Your emphasis on
            interoperability across heterogeneous platforms is also accurate; a key design goal for KQML and later ACLs
            was enabling communication among distributed agents with different internal architectures (Zhou et al., 2023).
          </p>

          <p>
            You identified several limitations, particularly the reasoning overhead and debugging difficulty. These issues
            often arise when agents lack shared semantic grounding. A practical mitigation strategy is the use of global
            ontology registries, which ensure that all agents operate with aligned concepts and meaning (Gruber, 1993).
            Another helpful approach is embedding conversation policies—structured communicative sequences that reduce
            ambiguity and guide how agents respond to misunderstandings.
          </p>

          <p>
            Your comparison with method invocation is correct: while Python or Java calls are fast, predictable, and
            tightly coupled, they do not support autonomous decision-making. Cardoso and Ferrando (2021) also argue that
            method invocation cannot scale effectively in open, dynamic multi-agent environments. However, method invocation
            remains useful for intra-agent operations or time-critical computations, where minimal overhead is necessary.
          </p>

          <p>
            Your analysis successfully demonstrates the architectural differences and highlights when each communication
            paradigm is most appropriate.
          </p>

          <div class="refs">
            <h4>References</h4>
            <ul>
              <li>Cardoso, R.C. and Ferrando, A. (2021) ‘A review of agent-based programming for multi-agent systems’, <em>Computers</em>, 10(2), p. 16.</li>
              <li>Gruber, T. (1993) ‘A translation approach…’, <em>Knowledge Acquisition</em>, 5(2), pp. 199–220.</li>
              <li>Qian, C. et al. (2023) ‘Communicative agents for software development’, arXiv:2307.07924.</li>
              <li>Zhou, W. et al. (2023) ‘Agents: An open-source framework…’, arXiv:2309.07870.</li>
              <li>Cheng, Y. et al. (2024) ‘Exploring large language model-based intelligent agents…’, arXiv:2401.03428.</li>
            </ul>
          </div>
        </div>
      </details>

      <details>
        <summary>
          <span>My Summary Post</span>
          <span class="summary-right">08 Dec 2025 • 11:04 AM</span>
        </summary>
        <div class="detail-body prose">
          <div class="kv">
            <div>Author</div><div>Naeema Alnaqbi</div>
            <div>Type</div><div>Summary Post</div>
          </div>

          <p>
            This discussion highlighted the strengths and limitations of Agent Communication Languages (ACLs), such as KQML
            and FIPA-ACL, and clarified their distinction from traditional method invocation. In my initial post, I
            emphasised how ACLs enable communication at the knowledge level, where agents exchange performatives such as ask,
            tell or achieve to express intent rather than merely transfer data (Finin et al., 1994). This semantic richness
            closely aligns with contemporary agent models such as the Belief-Desire-Intention (BDI) architecture, which
            depends on deliberation and goal-directed reasoning (Rao and Georgeff, 1995; Wooldridge, 2009).
          </p>

          <p>
            Feedback from peers strengthened this understanding. Ariel’s post highlighted the importance of decoupling intent
            from implementation, a principle that supports autonomy and traceability in multi-agent systems. This resonates
            with research showing that ACLs are essential for coordinating complex, decentralised workflows (Tsochev,
            Trifonov and Naydenov, 2015). Yousif’s contribution reinforced another key strength: interoperability. ACLs
            enable agents written in different languages or operating on different platforms to collaborate, making them
            suitable for distributed AI and cyber-physical environments (Zhou et al., 2023; Cardoso and Ferrando, 2021).
          </p>

          <p>
            Peers also raised concerns about performance overhead and semantic ambiguity. These issues are well-documented
            in the literature, where ontology misalignment and inconsistent interpretation are cited as major challenges
            (Chaib-Draa and Dignum, 2002). Preventive measures include formalised ontology engineering (Gruber, 1993) and the
            adoption of structured interaction protocols such as FIPA standards.
          </p>

          <p>
            By comparison, method invocation in Python or Java remains faster, simpler and easier to debug (Weiss, 1999).
            However, its tight coupling and lack of semantic expressiveness limit its suitability for autonomous, adaptive
            or negotiation-driven systems.
          </p>

          <p>
            Overall, the discussion reinforced that ACLs, despite their complexity, remain essential for intelligent
            multi-agent collaboration in dynamic environments.
          </p>

          <div class="refs">
            <h4>References</h4>
            <ul>
              <li>Cardoso, R.C. and Ferrando, A. (2021) ‘A review…’, <em>Computers</em>, 10(2), p.16.</li>
              <li>Chaib-Draa, B. and Dignum, F. (2002) ‘Trends in agent communication language’, <em>Computational Intelligence</em>, 18(2), pp. 89–101.</li>
              <li>Finin, T., Labrou, Y. and Mayfield, J. (1994) ‘KQML as an agent communication language’, in <em>CIKM</em>. New York: ACM, pp. 456–463.</li>
              <li>Gruber, T. (1993) ‘A translation approach…’, <em>Knowledge Acquisition</em>, 5(2), pp. 199–220.</li>
              <li>Rao, A.S. and Georgeff, M.P. (1995) ‘BDI agents…’, in <em>ICMAS’95</em>.</li>
              <li>Tsochev, G.R., Trifonov, R.I. and Naydenov, G.A. (2015) ‘Agent communication languages comparison…’, in <em>Computer Science 2015</em>.</li>
              <li>Weiss, G. (1999) <em>Multiagent Systems: A Modern Approach to Distributed Artificial Intelligence</em>. Cambridge, MA: MIT Press.</li>
              <li>Wooldridge, M. (2009) <em>An Introduction to Multi-Agent Systems</em>. 2nd edn. Chichester: Wiley.</li>
              <li>Zhou, W. et al. (2023) ‘Agents: An open-source framework…’, arXiv:2309.07870.</li>
            </ul>
          </div>
        </div>
      </details>
    </div>

    <hr class="sep" />

    <h2 class="section-title">Discussion 3</h2>
    <div class="card">
      <h3>Ethical Issues in Deep Learning–Enabled Generative Technologies</h3>

      <details open>
        <summary>
          <span>My Initial Post</span>
          <span class="summary-right">24 Jan 2026 • 4:10 PM</span>
        </summary>
        <div class="detail-body prose">
          <div class="kv">
            <div>Author</div><div>Naeema Alnaqbi</div>
            <div>Type</div><div>Initial Post</div>
          </div>

          <p>
            Deep learning–enabled generative technologies, such as ChatGPT for text generation and DALL·E for image creation,
            raise significant ethical issues that must be carefully considered. While these tools offer clear benefits in
            creativity, productivity, and problem-solving, their ability to generate convincing content at scale introduces
            predictable risks.
          </p>

          <p>
            One major ethical concern is bias and fairness. Deep learning models are trained on large datasets that often
            contain historical and societal biases. As a result, generated outputs may unintentionally reinforce stereotypes
            or marginalise certain groups, particularly when deployed in sensitive domains such as healthcare, education, or
            recruitment (Bolukbasi et al., 2016). Because these systems operate at scale, even subtle biases can be amplified
            and normalised.
          </p>

          <p>
            A second issue is misinformation and misuse. Generative models can produce highly fluent and realistic text or
            media that appears credible even when it is factually incorrect. This capability can be exploited to create
            deepfakes or spread disinformation, undermining public trust in digital information and democratic processes
            (Chesney and Citron, 2019).
          </p>

          <p>
            Intellectual property and authorship also present ethical challenges. Generative models are trained on vast
            amounts of existing content, often without explicit consent from creators. This raises questions about ownership,
            originality, and academic integrity, particularly when AI-generated work is presented as entirely human-authored
            (Floridi et al., 2018).
          </p>

          <p>
            Finally, there are concerns around transparency and accountability. Many deep learning systems function as “black
            boxes,” making it difficult to explain outputs or assign responsibility when harm occurs (Floridi et al., 2018).
          </p>

          <p>
            Overall, while generative deep learning technologies offer significant value, their ethical risks are real and
            foreseeable. Responsible deployment requires governance frameworks, technical safeguards, and context-aware use
            rather than unrestricted adoption.
          </p>

          <div class="refs">
            <h4>References</h4>
            <ul>
              <li>Bolukbasi, T. et al. (2016) ‘Man is to computer programmer as woman is to homemaker? Debiasing word embeddings’, <em>NeurIPS</em>, 29, pp. 4349–4357.</li>
              <li>Chesney, R. and Citron, D. (2019) ‘Deepfakes: A looming challenge…’, <em>California Law Review</em>, 107(6), pp. 1753–1820.</li>
              <li>Floridi, L. et al. (2018) ‘AI4People—An ethical framework for a good AI society’, <em>Minds and Machines</em>, 28(4), pp. 689–707.</li>
            </ul>
          </div>
        </div>
      </details>

      <details>
        <summary>
          <span>My Peer Response (to Abdulla Al Mutawa)</span>
          <span class="summary-right">24 Jan 2026 • 4:10 PM</span>
        </summary>
        <div class="detail-body prose">
          <div class="kv">
            <div>To</div><div>Abdulla Al Mutawa</div>
            <div>Type</div><div>Peer Response</div>
          </div>

          <p>Hi Abdulla,</p>

          <p>
            Your post clearly demonstrates that ethical risks such as bias, intellectual property, and misinformation are
            not accidental but structural outcomes of deploying generative models at scale. I agree, and I believe that
            effective prevention requires embedding safeguards throughout the development and deployment lifecycle rather
            than relying on post-hoc mitigation.
          </p>

          <p>
            Regarding bias amplification, one important preventative measure is mandatory pre-deployment auditing.
            Structured bias evaluations, combined with adversarial testing and documentation such as model cards, can help
            identify harmful patterns before systems are released (Bender et al., 2021). This is particularly important in
            high-impact domains where biased outputs may translate into real-world discrimination.
          </p>

          <p>
            In relation to misinformation and the “liar’s dividend”, technical solutions such as watermarking and
            provenance metadata should be complemented by platform-level governance. While detection alone is insufficient,
            combining traceable content credentials with institutional policies that prioritise verified sources can reduce
            the spread of unverified AI-generated content (Jobin et al., 2019).
          </p>

          <p>
            Finally, the intellectual property “value gap” you describe can be mitigated through stronger data governance.
            Requiring documentation of data provenance, licensed datasets, and opt-out mechanisms shifts responsibility
            toward developers and supports accountability in the event of disputes (Henderson et al., 2023).
          </p>

          <p>
            Overall, your argument effectively highlights that ethical frameworks must be integrated into system design
            rather than applied retrospectively.
          </p>

          <div class="refs">
            <h4>References</h4>
            <ul>
              <li>Bender, E.M. et al. (2021) ‘On the dangers of stochastic parrots…’, <em>FAccT</em>, pp. 610–623.</li>
              <li>Henderson, P. et al. (2023) ‘Foundation Models and Fair Use’, <em>JMLR</em>, 24(1), pp. 1–30.</li>
              <li>Jobin, A., Ienca, M. and Vayena, E. (2019) ‘The global landscape of AI ethics guidelines’, <em>Nature Machine Intelligence</em>, 1(9), pp. 389–399.</li>
            </ul>
          </div>
        </div>
      </details>

      <details>
        <summary>
          <span>My Peer Response (to Ariel Mella)</span>
          <span class="summary-right">24 Jan 2026 • 4:13 PM</span>
        </summary>
        <div class="detail-body prose">
          <div class="kv">
            <div>To</div><div>Ariel Mella</div>
            <div>Type</div><div>Peer Response</div>
          </div>

          <p>Hi Ariel,</p>

          <p>
            Your post provides a valuable perspective by linking deep learning ethics to real-world deployment examples,
            particularly customer service automation. I agree that one of the most significant risks of large language
            models is their tendency to generate confident but inaccurate information due to their probabilistic nature.
          </p>

          <p>
            A key preventative strategy is constraining model outputs in high-stakes contexts. Techniques such as
            retrieval-augmented generation and source-grounded responses can reduce hallucinations by requiring models to
            rely on verified information. Where grounding is not possible, systems should default to uncertainty disclosure
            and escalation to human oversight rather than producing speculative answers (Shanahan, 2024).
          </p>

          <p>
            Additionally, ethical deployment requires organisational governance, not just technical improvements. This
            includes clear usage policies, human-in-the-loop review in regulated sectors, and continuous post-deployment
            monitoring to detect emerging risks (Jobin et al., 2019). These measures help prevent incidents such as the
            example you cited, where automated systems were treated as authoritative sources.
          </p>

          <p>
            Finally, regarding workforce displacement, a preventative approach involves redesigning roles around AI-assisted
            decision-making rather than full automation. This reduces social harm while allowing organisations to benefit
            from efficiency gains.
          </p>

          <p>
            Your post effectively demonstrates that ethical risk arises not from model capability alone, but from how
            systems are integrated into real-world workflows.
          </p>

          <div class="refs">
            <h4>References</h4>
            <ul>
              <li>Jobin, A., Ienca, M. and Vayena, E. (2019) ‘The global landscape of AI ethics guidelines’, <em>Nature Machine Intelligence</em>, 1(9), pp. 389–399.</li>
              <li>Shanahan, M. (2024) ‘Talking about large language models’, <em>Communications of the ACM</em>, 67(2), pp. 68–79.</li>
            </ul>
          </div>
        </div>
      </details>

      <details>
        <summary>
          <span>My Summary Post</span>
          <span class="summary-right">24 Jan 2026 • 4:15 PM</span>
        </summary>
        <div class="detail-body prose">
          <div class="kv">
            <div>Author</div><div>Naeema Alnaqbi</div>
            <div>Type</div><div>Summary Post</div>
          </div>

          <p>
            This discussion across Units 9–11 highlights strong agreement that deep learning–based generative technologies
            present both substantial benefits and serious ethical challenges. In my initial post, I identified four core
            issues: bias and fairness, misinformation and misuse, intellectual property concerns, and the lack of
            transparency and accountability in deep learning systems (Floridi et al., 2018).
          </p>

          <p>
            Peer contributions expanded these themes with practical and technical insights. Several posts emphasised that
            bias is not merely inherited from training data but amplified through large-scale deployment, particularly in
            sensitive contexts such as healthcare and recruitment (Bolukbasi et al., 2016). Others highlighted the growing
            risk of misinformation, noting that deepfakes and fluent but inaccurate text threaten public trust and create an
            ongoing arms race between content generation and detection (Chesney and Citron, 2019).
          </p>

          <p>
            The discussion around intellectual property revealed that legal uncertainty remains unresolved. The concept of a
            “value gap,” where AI systems monetise human creativity without clear compensation, reinforced the need for
            stronger data governance and clearer attribution mechanisms. Importantly, peers argued that these issues cannot
            be addressed through technical solutions alone.
          </p>

          <p>
            A key learning outcome from this forum is that ethical AI requires an integrated approach. Technical safeguards
            such as auditing and grounding must be combined with governance frameworks, institutional policies, and human
            oversight (Jobin et al., 2019). Ethical considerations should be embedded throughout the AI lifecycle rather
            than treated as an afterthought.
          </p>

          <p>
            Overall, the discussion demonstrates that responsible use of generative deep learning is not about restricting
            innovation, but about guiding it carefully. Addressing bias, misinformation, ownership, and accountability
            together is essential if these technologies are to deliver societal benefit without causing long-term harm.
          </p>

          <div class="refs">
            <h4>References</h4>
            <ul>
              <li>Bolukbasi, T. et al. (2016) ‘Debiasing word embeddings’, <em>NeurIPS</em>, 29, pp. 4349–4357.</li>
              <li>Chesney, R. and Citron, D. (2019) ‘Deepfakes…’, <em>California Law Review</em>, 107(6), pp. 1753–1820.</li>
              <li>Floridi, L. et al. (2018) ‘AI4People…’, <em>Minds and Machines</em>, 28(4), pp. 689–707.</li>
              <li>Jobin, A., Ienca, M. and Vayena, E. (2019) ‘AI ethics guidelines’, <em>Nature Machine Intelligence</em>, 1(9), pp. 389–399.</li>
            </ul>
          </div>
        </div>
      </details>
    </div>

    <footer class="footer">
      <span>© Naeema Alnaqbi • Intelligent Agents e-Portfolio</span>
      <a class="toplink" href="#top">Back to top</a>
    </footer>
  </main>
</body>
</html>
